{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use ORCA\n",
    "\n",
    "ORCA is an experimental framework focused on productivity and experiments reproducibility for machine learning researchers. Although initially created to collect ordinal classification methods, it is also suitable for other classifiers.\n",
    "\n",
    "First, you will need to install the framework. To do so, please visit [ORCA Quick Install Guide](orca-quick-install.md). Note that you should be able to perform the test when the framework is successfully installed.\n",
    "\n",
    "This tutorial uses tree small datasets (`pasture`, `tae`, `toy`) contained in folder [example data](../exampledata/30-holdout). The datasets are already partitioned with a 30-holdout experimental design.\n",
    "\n",
    "This tutorial is prepared for running the experiments in Matlab, although it should be easily adapted to Octave. \n",
    "\n",
    "*NOTE:*\n",
    "\n",
    "Small datasets like the ones used in this tutorial usually produce warning messages such as:\n",
    "```MATLAB\n",
    "Warning: Matrix is close to singular or badly scaled. Results may be inaccurate. RCOND =\n",
    "1.747151e-17.\n",
    "Warning: Maximum likelihood estimation did not converge.  Iteration limit\n",
    "exceeded.  You may need to merge categories to increase observed counts.\n",
    "\n",
    "```\n",
    "\n",
    "You can disable these messages by using the following code in Matlab:\n",
    "```MATLAB\n",
    "warning('off','MATLAB:nearlySingularMatrix')\n",
    "warning('off','stats:mnrfit:IterOrEvalLimit')\n",
    "```\n",
    "\n",
    "In Octave, to disable `warning: matrix singular to machine precision` we need to disable all the warnings: \n",
    "```MATLAB\n",
    "warning('off','all');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch experiments through `ini` files\n",
    "\n",
    "In this section, we run several experiments to compare the performance of three methods in a set of datasets: POM (Proportional Odds Model) [1], SVORIM (Support Vector Machines with IMplicit constrains) [2] and SVC1V1 (SVM classifier with 1-vs-1 binary decomposition) [3]. POM is a linear ordinal model, with limited performance but easy interpretation. SVORIM is an ordinal nonlinear model, with one of the most competitive performances according to several studies. SVC1V1 is the nominal counterpart of SVORIM, so that we can check the benefits of considering the order of the classes. To learn more about ordinal performance metrics see [4].\n",
    "\n",
    "From the Octave console load packages and add orca files to path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Install dataframe, add src to path and disable some warnings\n",
    "pkg install -forge dataframe\n",
    "warning('off','all');\n",
    "\n",
    "addpath('../src/Algorithms/')\n",
    "addpath('../src/Measures/')\n",
    "addpath('../src/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of experiments described in INI file `tutorial/config-files/pom.ini` can be run by (the syntax of these files will be explained in the [next subsection](#Syntax-of-ini-files)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utilities.runExperiments('tutorial/config-files/pom.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, ORCA analyses all the files included in the folder of the dataset, where training and test partitions are included (a pair of files `train_dataset.X` and `test_dataset.X` for each dataset, where `X` is the number of partition). For each partition, a model is trained on training data and tested on test data.\n",
    "\n",
    "After this, you can also run the experiments for SVORIM and SVC1V1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utilities.runExperiments('tutorial/config-files/svorim.ini')\n",
    "Utilities.runExperiments('tutorial/config-files/svc1v1.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the experiments are finished, the corresponding results can be found in the `Experiments` subfolder, as described in the [corresponding section](#Experimental-results-and-reports) of this tutorial.\n",
    "\n",
    "Each experiment has a different folder, and each folder should include two CSV files with results similar to the following (some columns are omitted):\n",
    "\n",
    "POM results ([download CSV](tutorial/reference-results/pom-mean-results_test.csv)):\n",
    "\n",
    "| Dataset-Experiment | MeanMAE | StdMAE | MeanAMAE | StdAMAE | MeanTrainTime | StdTrainTime |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| pasture-pom-tutorial | 0.6 | 0.230866 | 0.6 | 0.230866 | 0.070958 | 0.004822 |\n",
    "| tae-pom-tutorial | 0.615789 | 0.100766 | 0.616952 | 0.101876 | 0.324884 | 0.087447 |\n",
    "| toy-pom-tutorial | 0.980889 | 0.038941 | 1.213242 | 0.059357 | 0.038949 | 0.002738 |\n",
    "\n",
    "SVORIM results ([download CSV](tutorial/reference-results/svorim-mean-results_test.csv)):\n",
    "\n",
    "| Dataset-Experiment | MeanMAE | StdMAE | MeanAMAE | StdAMAE | MeanTrainTime | StdTrainTime |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| pasture-svorim-mae-real | 0.322222 | 0.106614 | 0.322222 | 0.106614 | 0.013843 | 0.002601 |\n",
    "| tae-svorim-mae-real | 0.475439 | 0.069086 | 0.473291 | 0.068956 | 0.042999 | 0.023227 |\n",
    "| toy-svorim-mae-real | 0.017778 | 0.012786 | 0.019631 | 0.015726 | 0.071385 | 0.025767 |\n",
    "\n",
    "SVC1V1 results ([download CSV](tutorial/reference-results/svc1v1-mean-results_test.csv)):\n",
    "\n",
    "| Dataset-Experiment | MeanMAE | StdMAE | MeanAMAE | StdAMAE | MeanTrainTime | StdTrainTime |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| pasture-svc1v1-mae-tutorial | 0.314815 | 0.127468 | 0.314815 | 0.127468 | 0.014363 | 0.003297 |\n",
    "| tae-svc1v1-mae-tutorial | 0.534211 | 0.108865 | 0.533832 | 0.110083 | 0.017699 | 0.004122 |\n",
    "| toy-svc1v1-mae-tutorial | 0.051556 | 0.023419 | 0.044367 | 0.022971 | 0.015869 | 0.003786 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Exercise 1***: apparently, POM is the slowest method, but here we are not considering the crossvalidation time. Check the detailed CSV results to conclude which is the method with the lowest computational cost (taking crossvalidation, training and test phases into account).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can plot a bar plot to graphically compare the performance of the methods. Let's analyse for that the `toy` dataset. This is a synthetic dataset proposed by Herbrich et al. in their paper \"Support vector learning for ordinal regression\" (1997):\n",
    "![Synthetic toy dataset](tutorial/images/toy.png)\n",
    "\n",
    "The following code (to be run from the `src` folder) plots the figure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg load dataframe\n",
    "pomT = dataframe('../doc/tutorial/reference-results/pom-mean-results_test.csv');\n",
    "svorimT = dataframe('../doc/tutorial/reference-results/svorim-mean-results_test.csv');\n",
    "svc1v1T = dataframe('../doc/tutorial/reference-results/svc1v1-mean-results_test.csv');\n",
    "datasets = {'pasture','tae','toy'};\n",
    "\n",
    "bar([pomT.MeanAMAE svorimT.MeanAMAE svc1v1T.MeanAMAE])\n",
    "set (gca, 'xticklabel',datasets)\n",
    "legend('POM  ', 'SVORIM', 'SVC1V1')\n",
    "title('AMAE performance (smaller is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "***Exercise 2***: you can repeat this barplot but now considering:\n",
    "- A `global` (i.e. a metric where the class a priori probability is not considered) **nominal** metric.\n",
    "- A `global` **ordinal** metric.\n",
    "- A **nominal** metric specifically designed for imbalanced datasets.\n",
    "- An **ordinal** metric specifically designed for imbalanced datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax of `ini` files\n",
    "\n",
    "ORCA experiments are specified in configuration `ini` files, which execute an algorithm for a collection of datasets (each dataset with a given number of partitions). The folder [src/config-files](../src/config-files) contains example configuration files for running all the algorithms included in ORCA for all the algorithms and datasets of the [review paper](http://www.uco.es/grupos/ayrna/orreview). The following code is an example for running the Proportion Odds Model (POM), a.k.a. Ordinal Logistic Regression. Note that the execution of this `ini` file can take several hours:\n",
    "```INI\n",
    "; Experiment ID\n",
    "[pom-real\n",
    "{general-conf}\n",
    "seed = 1\n",
    "; Datasets path\n",
    "basedir = ../../../datasets/ordinal/real/30-holdout\n",
    "; Datasets to process (comma separated list or 'all' to process all)\n",
    "datasets = automobile,balance-scale,bondrate,car,contact-lenses,ERA,ESL,eucalyptus,LEV,marketing,newthyroid,pasture,squash-stored,squash-unstored,SWD,tae,thyroid,toy,winequality-red,winequality-white\n",
    "; Activate data standardization\n",
    "standarize = true\n",
    "\n",
    "; Method: algorithm and parameter\n",
    "{algorithm-parameters}\n",
    "algorithm = POM\n",
    "```\n",
    "\n",
    "`ini` files include **Subsections** to help organize the configuration. These sections are mandatory:\n",
    " - `{general-conf}`: generic parameters of the experiment, including the seed considered for random number generation, the directory containing the datasets, the datasets to be processed... All the parameters included here are the same for all the algorithms.\n",
    " - `{algorithm-parameters}`: here you can specify the algorithm to run and the parameters which are going to be fixed (not optimized through cross validation).\n",
    " - `{algorithm-hyper-parameters-to-cv}`: algorithms' hyper-parameters to optimise. For more details, see [Hyper-parameter optimization](#Hyper-parameter-optimization).\n",
    "\n",
    "The above file tells ORCA to run the algorithm `POM` for all the datasets specified in the list `datasets`. You can also use `datasets = all` to process all the datasets in `basedir`). Each of these datasets should be found at folder `basedir`, in such a way that ORCA expects one subfolder for each dataset, where the name of the subfolder must match the name of the dataset. Other directives are:\n",
    " - INI section `[pom-real]` sets the experiment identifier.\n",
    " - The `standarize` flag activates the standardization of the data (by using the mean and standard deviation of the train set).\n",
    " - The rest of the parameters of the model depend on the specific algorithm (and they should be checked in the documentation of the algorithm). For instance, the kernel type is set up with `kernel` parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter optimization\n",
    "\n",
    "Many machine learning methods are very sensitive to the value considered for the hyper-parameters (consider, for example, support vector machines and the two associated parameters, cost and kernel width). ORCA automates hyper-parameter optimization by using a grid search with an internal nested *k*-fold cross-validation considering only the training partition. Let see an example for the optimisation of the two hyper-parameters of SVORIM: cost (`C`) and kernel width parameter (`k`, a.k.a. *gamma*):\n",
    "```ini\n",
    "; Experiment ID\n",
    "[svorim-mae-real]\n",
    "{general-conf}\n",
    "seed = 1\n",
    "; Datasets path\n",
    "basedir = datasets/ordinal/real/30-holdout\n",
    "; Datasets to process (comma separated list)\n",
    "datasets = all\n",
    "; Activate data standardization\n",
    "standarize = true\n",
    "; Number of folds for the parameters optimization\n",
    "num_folds = 5\n",
    "; Crossvalidation metric\n",
    "cvmetric = mae\n",
    "\n",
    "; Method: algorithm and parameter\n",
    "{algorithm-parameters}\n",
    "algorithm = SVORIM\n",
    "kernel = rbf\n",
    "\n",
    "; Method's hyper-parameter values to optimize\n",
    "{algorithm-hyper-parameters-to-cv}\n",
    "C = 10.^(-3:1:3)\n",
    "k = 10.^(-3:1:3)\n",
    "```\n",
    "\n",
    "The directive for configuring the search process is included in the general section. The directives associated to hyper-parameter optimisation are:\n",
    "- `seed`: is the value to initialize MATLAB random number generator. This can be helpful to debug algorithms.\n",
    "- `num_folds`: *k* value for the nested *k*-fold cross validation over the training data.\n",
    "- `cvmetric`: metric used to select the best hyper-parameters in the grid search. The metrics available are: `AMAE`,`CCR`,`GM`,`MAE`,`MMAE`,`MS`,`MZE`,`Spearman`,`Tkendall` and `Wkappa`.\n",
    "- The list of hyper-parameters to be optimised and values considered for each parameter during the grid search are specified in subsection `{algorithm-hyper-parameters-to-cv}`;\n",
    "    - `C`: add a new parameter with name `C` and a set of values of `10.^(-3:1:3)` (10<sup>-3</sup>,10<sup>-2</sup>,...,10<sup>3</sup>). The same apples for `k`.\n",
    "\n",
    "The parameter optimization can also be done by using the API (full example is in [exampleParamOptimization.m](../src/code-examples/exampleParamOptimization.m) script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Load the different partitions of the dataset\n",
    "load ../exampledata/1-holdout/toy/matlab/train_toy.0\n",
    "load ../exampledata/1-holdout/toy/matlab/test_toy.0\n",
    "\n",
    "% \"patterns\" refers to the input variables and targets to the output one\n",
    "train.patterns = train_toy(:,1:end-1);\n",
    "train.targets = train_toy(:,end);\n",
    "test.patterns = test_toy(:,1:end-1);\n",
    "test.targets = test_toy(:,end);\n",
    "\n",
    "% Assumes training set in structure 'train'\n",
    "% Create the algorithm object\n",
    "algorithmObj = KDLOR();\n",
    "% Create vectors of values to test\n",
    "param.C = 10.^(-3:1:3);\n",
    "param.k = 10.^(-3:1:3);\n",
    "param.u = [0.01,0.001,0.0001,0.00001];\n",
    "\n",
    "% Optimizing parameters for KDLOR with metric MAE (default metric)\n",
    "optimalp = paramopt(algorithmObj,param,train)\n",
    "\n",
    "% Optimizing parameters for KDLOR with metric GM\n",
    "optimalp = paramopt(algorithmObj,param,train, 'metric', GM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental results and reports\n",
    "\n",
    "ORCA uses the `Experiments` folder to store all the results of the different experiments. Each report is placed in a subfolder of `Experiments` named with the current date, time and the name of the configuration file (for example 'exp-2015-7-14-10-5-57-pom'). After a successful experiment, this folder should contain the following information:\n",
    " - Individual experiment configuration files for each dataset and partition.\n",
    " - A `Results` folder with the following information:\n",
    "    - `mean-results_train.csv` and `mean-results_test.csv` which are the reports in CSV format (easily read by Excel or LibreOffice Calc). They contain the mean and standard deviation for each performance measure (`AMAE`,`CCR`,`GM`,`MAE`,`MMAE`,`MS`,`MZE`,`Spearman`,`Tkendall` and `Wkappa`) and the computational time. These averages and standard deviations are obtained for all the partitions of each algorithm and dataset.\n",
    "    - The `Results` folder contains one subfolder for each dataset with the following data:\n",
    "        - Train and test confusion matrices (`matrices.txt`).\n",
    "        - Name of the folder used for the experiments (`dataset`).\n",
    "        - Individual results for each partition in CSV format (`results.csv`).\n",
    "        - Models of each partition in `.mat` format (`Models` folder). These models are structures and their fields depend on the specific algorithm.\n",
    "        - Decision values used to obtain the predicted labels for training and test partitions ('Guess' folder). For threshold models, this is the one dimensional mapping before applying the discretisation based on the thresholds. The rest of models may have multidimensional mappings.\n",
    "        - Labels predicted by the models for each partition ('Predictions' folder).\n",
    "        - Optimal hyper-parameters values obtained after nested cross-validation ('OptHyperparams').\n",
    "        - Computational time results ('Times').\n",
    "\n",
    "If you provide the option `report_sum = true` in `{general-conf}`, additionally the same metrics will be calculated with a matrix that is the sum of the generalization matrices (as Weka does). **Note that this only makes sense in the case of a k-fold experimental design**. With this option active, two additional reports will be generated (`mean-results_matrices_sum_train.csv` and `mean-results_matrices_sum_test.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running algorithms with ORCA API\n",
    "\n",
    "### Run a pair of train-test files with fitpredict\n",
    "\n",
    "ORCA algorithms can be used from your own Matlab code. All algorithms included in the [Algorithms](../src/Algorithms) have a `fitpredict` method, which can be used for running the algorithms with your data. The method receives a structure with the matrix of training data and labels, the equivalent for test data and a structure with the values of the parameters associated to the method. With respect to other tools, parameters are a mandatory argument for the method to avoid the use of default values.\n",
    "\n",
    "For example, the [KDLOR (Kernel Discriminant Learning for Ordinal Regression)](../src/Algorithms/KDLOR.m) [5]  method has a total of five parameters. Two of them (the type of kernel, `kernelType`, and the optimisation routine considered, `optimizationMethod`) are received in the constructor of the corresponding class, and the other three parameters (cost, `C`, kernel parameter, `k`, and value to avoid singularities, `u`) are supposed to have to be fine-tuned for each dataset and partition, so they are received in a structure passed to the `fitpredict` method.\n",
    "\n",
    "This an example of execution of KDLOR from the Matlab console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addpath('../src/Algorithms/')\n",
    "kdlorAlgorithm = KDLOR('kernelType','rbf','optimizationMethod','quadprog');\n",
    "kdlorAlgorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load ../exampledata/1-holdout/toy/matlab/train_toy.0\n",
    "load ../exampledata/1-holdout/toy/matlab/test_toy.0\n",
    "train.patterns = train_toy(:,1:(size(train_toy,2)-1));\n",
    "train.targets = train_toy(:,size(train_toy,2));\n",
    "test.patterns = test_toy(:,1:(size(test_toy,2)-1));\n",
    "test.targets = test_toy(:,size(test_toy,2));\n",
    "param.C = 10;\n",
    "param.k = 0.1;\n",
    "param.u = 0.001;\n",
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model and test prediction with generalization data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = kdlorAlgorithm.fitpredict(train,test,param);\n",
    "fieldnames(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprintf('Accuracy Train %f, Accuracy Test %f\\n',\n",
    "    sum(train.targets==info.predictedTrain)/size(train.targets,1),\n",
    "    sum(test.targets==info.predictedTest)/size(test.targets,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the methods return a structure with the main information about the execution of the algorithm. The fields of this structure are:\n",
    "- `projectedTrain`: decision values for the training set.\n",
    "- `predictedTrain`: labels predicted for the training set.\n",
    "- `trainTime`: time in seconds needed for training the model.\n",
    "- `projectedTest`: decision values for the test set.\n",
    "- `predictedTest`: labels predicted for the test set.\n",
    "- `testTime`: time in seconds needed for the test phase.\n",
    "- `model`: structure containing the model (its coefficients, parameters, etc.). Note that, although most of the fields of this structure depend on the specific algorithm considered, we will always find the `algorithm` and `parameters` fields. These are the fields for KDLOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames(info.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e., the algorithm used for training (`algorithm`), the weight given to each pattern in the kernel model (`projection`), the set of threshold values (`thresholds`), the parameters used for training (`parameters`), the type of kernel considered (`kernelType`) and the training data (`train`). As can be checked, at least, this structure should contain the information for performing the test phase. In this way, for KDLOR, the prediction phase needs to apply the kernel to each training point and the test point being evaluated (using `kernelType`, `train` and `parameters.K`) and perform the weighted sum of these values (using `projection`). After that, the thresholds are used to obtain the labels.\n",
    "\n",
    "The corresponding script ([exampleKDLOR.m](../src/code-examples/exampleKDLOR.m)) can be found and run in the [code example](../src/code-examples) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmpath('../src/Measures')\n",
    "rmpath('../src/Algorithms')\n",
    "rmpath('../src/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
